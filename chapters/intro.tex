%1) Big data, efficient evaluation, linear time over the data. 
%
%2) Query evaluation, query languages Q, model this as Q(D). Evaluation in |D|.
%
%3) Q(D) is only one part of the story, also we need to talk about results. You want to evaluate and enumerate the output hopefully in time f(Q,D) + |O|.
%
%4) Talk about constant delay as a more efficient notion of enumeration guarantee.
%
%5) Let's instatiate this idea in a more concrete query language, specifically, in this work a good starting point is to talk about query lanuguages based on MSO. Hablar sobre MSO, XML, JSON, bases de datos de grafos, etc.
%
%6) Spanners como un ejemplo especial y importante para este trabajo. 
%
%7) (parráfo corto) Concluir que en todos estos casos, al final, la consulta se puede expresar como un automata (en algún modelo) y los datos como un string. 
%
%8) Evaluación eficient con enumeración de automatas ya se ha estudiado harto y teniendo grandes resultados. Por ejemplo, Bagan, Antoine, Spanners, CEP. 
%
%9) Todos los resultados anteriores no consideran algún nivel de recursión en los automatas como visibly, grammars. Es sabido que visibly y gramaticas también tienen buenas propiedades algoritmicas, pero esto es solo para el caso booleano. Creemos que podemos estos resultados a automatas o datos donde hay cierto nivel de recursión.

The term {\it Big Data} is believed to have been originated in the mid-1990s, back when the Internet was utilized by some tens of millions of users. Even then, as the usage of the term suggests, there was a distinct necessity of processing tremendous amounts of data efficiently. In 2023, this number grew to 5,4 billion users~\cite{netuse}, and that has naturally brought a great increase in the amount of raw data that is available and required to be processed.
An example of online data task that has become commonplace are recommendation algorithms. Nowadays, four out of the five most visited sites on the Internet nowadays use user-generated information to suggest content back to the users~\cite{topwebsites}. 
To name a few numbers, Netflix reported in 2012 that their users queued 2 million movies and TV shows, and generated 4 million ratings a day~\cite{netflixsize}, all of which is data that feeds into their recommendation algorithms.   
Another task is content management and moderation. In contexts where millions of messages are being sent daily, these are considered crucial to curb undesirable data such as fake news~\cite{fakenews} and hate speech~\cite{hatespeech}.
These cases show how overreaching, diverse and massive the online data processing tasks have become.

The blowup of large data processing has happened in the offline world as well. In biological research, human genomes consist of around three billion base pairs, which translate into several gigabytes of raw data that is processed when performing genome-matching or detecting genetic diseases~\cite{genomes}. Modern space missions, such as GAIA and LSST, are expected to produce Petabytes of data~\cite{space}. To name a few other examples of offline data processing, patents are analyzed in bulk in order to predict future litigations~\cite{patents}; GPS records from large populations can be used in urban planning projects~\cite{urban}; and historical crime data is massively processed to predict crime~\cite{crime}.

The importance and desirability of having adequate data processing solutions is unquestionable. In many cases, these algorithms need to be sharply efficient, to the point of making any solution that takes time longer than linear in the data size useless in practice.

In database systems, it is common to talk about {\it query evaluation} as the most elementary task which extracts relevant information from data. It can be formally defined by an instance $(Q, D)$ in which $Q$ is the query and $D$ is the data. Each query $Q$ also defines a function which maps $D$ into the desired set of outputs $Q(D)$.
As an example, consider the task of detecting if a chromosome, written as a sequence of bases, contains a certain pattern in it that may correspond to a particular disease. In this case, it would make sense to let $D$ be the sequence of bases written as a string, and $Q$ would be the pattern one wants to find. If the task is a yes/no question -- is the pattern $Q$ present in $D$? -- then $Q(D)$ can evaluate to either {\it true} or {\it false}, but if one wants to also know all of the positions in the sequence that match that pattern, $Q(D)$ would be a set that contains all relevant positions, and in the case there are none, it evaluates to an empty set.
With this formalization, one can talk about linear-time solutions as those algorithms that take time proportional to the size of $D$ to compute $Q(D)$.

It is also useful to frame results around {\it query languages}, which define classes of queries by their syntax and semantics. For instance, one can solve the task above by using {\it regular expressions} as the query language (see~\cite{automatahandbook} for a precise definition): we can make $Q$ be the regular expression $r = \texttt{.}^\texttt{*}p\texttt{.}^\texttt{*}$ where $p$ is the pattern string. In this case, we are using a usual semantics for regular expressions which is that $r$ has to match the entire text; the sequence $\texttt{.}^\texttt{*}$ is a wildcard that matches any string, and so $r$ is a match if the data is equal to anything, followed by $p$, followed by anything. Another aspect of query languages that make them natural to work with is that each of them also comes with a certain expressive power. For example, no regular expression can define the query of checking if two strings are identical~\cite{automatahandbook}, whereas there are more powerful query languages that can. As evaluation algorithms are built for each particular query language, a weaker one can require less running time or less memory space in the worst case. It can happen that the same query becomes cheaper to compute if the user simply expresses it in a different language. 

When measuring the execution time of a evaluation algorithm, the classic notion is to count the number of steps taken by the algorithm from the moment it begins, until the last output is printed. Since algorithms are built for inputs of different sizes, we typically measure this execution time as a function of the size of the input.
This is a reasonable model whenever the overall output size is small, e.g., when the output is simply true or false, or a numeric value. However, for tasks in which one can expect a large amount of outputs, which occur quite naturally in many data management problems, it makes sense to treat the output printing phase separately. Naturally, the best execution time one can expect for this phase is linear in the size of the entire output set, so the overall running time that we will set as our goal is given by $|D| + |\text{Output}|$. Here we are using the standard notation $|x|$ to refer to the size of an adequate encoding of $x$.

In database systems, there are many cases in which an evaluation problem can have a large output set. For instance, one may be interested not only in all elements in the data that satisfy a certain condition, but in tuples or subsets of them. Or perhaps the data may not fit in memory: it could be given in a streaming fashion or it could be compressed, and this implies that even an output set that has size linear in the raw data might be huge. An obvious problem here is that producing all outputs can take unreasonably long, but a more subtle one is that when one measures the running time of an algorithm, the cost of writing the outputs hides the cost of actually doing the calculations to obtain them. For this reason, a significant line of research on query evaluation has adopted the perspective of {\it enumeration algorithms}~\cite{Bagan06}. Instead of explicitly producing all results, the task is to enumerate them, in any order and without repetition. The cost of the algorithm is then measured across two dimensions: the {\it preprocessing time}, which is the time needed to read the input and prepare an enumeration data structure; and the {\it delay}, the worst-case time that can elapse between any two solutions while enumerating using the data structure.


In the realm of enumeration algorithms, there is an even finer complexity yardstick:
We say that an algorithm has {\it constant-delay}~\cite{Segoufin13} if the delay between any two consecutive outputs is constant. One can think of this as an enumeration phase in which the outputs are produced in a fixed pace, never stopping until the last output is produced. However, this delay guarantee is not a reasonable one unless every output is expected to have constant size. Instead, in a lot of cases it makes more sense to set as a goal what we call {\it output-linear delay}~\cite{FlorenzanoRUVV20}. This is a relaxation of constant-delay which only requires the delay between two outputs to be linear in the size of the earlier one.

As an example, consider the task of receiving a list of numbers, such as $L = [5, 7, 2, 12]$, and producing a list of pairs $(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)$ in which $x_1,x_2,\ldots,x_n$ is the sorted list, and each $y_i$ is the difference between $x_i$ and $x_{i+1}$. For the said $L$, the desired set of outputs could be $\{(2,3), (5,2), (7,5),(12,\infty)\}$. A constant-delay algorithm may simply sort the list, read it sequentially and produce each value along with the difference. Now, consider the task of again receiving a list of numbers, but now instead of printing the difference between two numbers, we ask for the numbers that are present in that gap. For the input $L$, the output set would be $\{(2,3,4),(5,6),(7,8,9,10,11),(12,\infty)\}$. In this case, there is no reasonable way of solving this with a constant-delay algorithm, since the size of a single output could be arbitrarily large, even for a list of size 2. However, sorting the list, traversing it linearly and writing down the gaps as they are seen is a perfectly valid solution that satisfies output-linear delay.

One area where enumeration algorithms have been especially successful is the field of information extraction, where {\em document spanners} have been proposed as a suitable formalism~\cite{FaginKRV15}. Information extraction captures the family of data management tasks where the data is a document, i.e, a string of characters, and the query is a description of the information that should be extracted from the text. A spanner formalizes this idea by defining a function that maps documents to sets of {\em mappings}, which themselves map variables to substrings of the document (called {\em spans}). The enumeration problem is then to enumerate all mappings of a spanner on an input document.
The work by Florenzano et al.~\cite{FlorenzanoRUVV20} showed that, if the spanner is described by a finite automaton, then the task could be solved with output-linear delay after a preprocessing that is linear in the document, and polynomial in the spanner if the automaton is deterministic; this was extended in~\cite{amarilli2019constant} to allow similar bounds for nondeterministic finite automata.

Solving enumeration tasks is a fundamental problem in database systems, and this is the main topic of this thesis: finding efficient algorithms to evaluate queries over great volumes of data with output-linear delay. In particular, we work on this problem over nested documents or query languages over documents that allow recursion.

In many practical cases, the way these large volumes of data are contained is in documents that have a pre-established recursive structure. It may be hierarchical, like XML and JSON documents, or program-like, as in some forms of compressed text~\cite{LZ77, StorerS82, ClaudeN11}. These documents are designed to be read by computers; the way the user is expected to access the data is by writing a query using a suitable query language, and then process the query through a specialized evaluation algorithm.

Let us look at a simple XML document to show how data may be laid out in practice. The following document (separated in two columns) contains information about musical artists and albums satisfying the hierarchy {\tt Artist->Album->} {\tt [Title, Year]}:
	
\begin{multicols}{2}
	\begin{small}
	\begin{verbatim}
<Document>
  <Artist>
    <Name>Los Tres</Name>
    <Album>
      <Title>Los Tres</Title>
      <Year>1991</Year>
    </Album>
    <Album>
      <Title>Fome</Title>
      <Year>1997</Year>
    </Album>
  </Artist>
  <Artist>
    <Name>La Ley</Name>
    <Album>
      <Title>Invisible</Title>
      <Year>1995</Year>
    </Album>
  </Artist>
  <Artist>
    <Name>Tiro de Gracia</Name>
    <Album>
      <Title>Ser Humano!!</Title>
      <Year>1997</Year>
    </Album>
  </Artist>
</Document>
	\end{verbatim}
	\end{small}
\end{multicols}

It can be appreciated how XML owes its classification of ``semi-structured data'' to the fact that the contents can be still understood by the layperson, yet there is some machine-oriented structure to the data that a human user would find suprerfuous. In the example above, the entire document contains a list of artists, and the structure requires  the info of each to be demarcated by the tags {\tt <Document>} and {\tt </Document>}

%Some of query languages that were built to process XML documents are XQuery and blabla. These are some examples of the languages that we will be suited to work with, which we will present shortly.

In this thesis, we will sometimes restrict ourselves to evaluation tasks where the data part of the query is presented with certain recursive structures. Those being, nested documents, and program-like documents. For some tasks, we allow a more standard text structure.

Let us now discuss the presentation aspect for the {\em query} part of a task. That is, the query languages that we will deal with in this thesis: these are all extensions to a logic called Monadic Second Order Logic. The logic in itself is of immense theoretical importance, but in actual data extraction tasks it is rarely used as-is. The reader might be more familiar with a query language which is equivalent in expressive power to Monadic Second Order over documents---that is, regular expressions. In database systems and data extraction tasks, regular expressions (regex for short) have long been a favoured choice for theoreticians, developers and users alike. 

When a single document is huge, and queries are simple enough, it makes sense to use a language based on regular expressions, such as XPath in the case of XML. 
Continuing the "albums" example above, an XPath query that produces the albums that were released in the year 1997 might look as follows: 
$$Q = \texttt{//Artist/Album[Year = 1997]}.$$
This would output info about albums {\it Fome} and {\it Ser Humano!!}.

In the case of compressed text, one can use a plain regular expression, and process it using some of the known algorithms that evaluate it on the compressed file itself~\cite{Lohrey12}. Among these solutions, one uses regular spanners (i.e. document spanners based on regular expressions) over compressed documents to enumerate with delay that is logarithmic in the size of the uncompressed document~\cite{SchmidS21}.

 
% Document Spanners [cita benny] is a specially relevant model for queries based on regular expressions. In the area of Information Extraction -- the field of blabl ablblablabla -- the sub-area of rule-based Information Extraction studies bla blabl bla bl bal, where Document Spanners were proposed as a suitable formalism.  In its more fundamental definition, a Document Spanner is a function that maps a document to an output set that contains tuples of {\it spans}, which are pairs of positions in the document. They can easily model tasks of converting plain or semi-structured text into a relational database. Inside these, we find {\it Regular Spanners}, which are defined by extending regular expressions (or finite-state automata) with symbols that mark the beginning and ending of a span. A more powerful model are the {\it Core Spanners}, which extend Regular Spanners with operators from relational algebra, such as projection, complement, selection of a spanner and joins between them. 
% 
% Efficient enumeration for queries based on Regular Spanners has found great success in relatively recent times. A relevant precursor was the result given in~\cite{bagan2006mso}, which describes how to enumerate the valid assignments for a given MSO formula --- onto which Regular Spanners can be trivially reduced. The papers~\cite{FlorenzanoRUVV20} and~\cite{amarilli2019constant} give parallel results which improve the delay to linear in the {\it support} of the assignments. The work in~\cite{Niewerth18} had a log-delay result for MSO queries over trees, which was later improved to output-linear delay~\cite{amarilli2019enumeration}. 
% 
There are evaluation tasks in which regular expressions are not powerful enough, and the data document does not have a pre-established structure. The immediate extension to regular expressions and MSO that also allows the query itself to infer this very structure are context-free grammars. Some scenarios in which this type of queries have been famously used are code parsing and verification on nested documents---both scenarios where the data itself would ideally satisfy a desired structure, but this is not a-priori guaranteed. 

Grammars by themselves do not describe how to capture substrings, so they are not immediately suited for extracting data. One model that has been proposed to bridge this gap is based on document spanners and is called Extraction Grammars~\cite{Peterfreund21}. These are defined by context-free grammars which allow beginning and ending position marks in its syntax. 
 In same work, we also find a relevant enumeration result. It shows constant-delay enumeration after a preprocessing that takes quintic time on the size of the document.
 

%Complex Event Processing is another realm that is relevant to our work. Here, the goal is to blablballbalbla. Some enumeration algorithms for queries in this area have also been realized by compiling them into a suitable automaton~\cite{GrezRU19,GrezR20,BucchiGQRV22}.


%Queries based on regular expressions and automata can be understood under the umbrella of Monadic Second Order logic. 
%A version of this logic that is restricted to work on documents would represent a document of size $n$ as a structure with domain $\{1,\ldots, n\}$ and predicates $P_a$ that evaluate $P_a(i)$ as {\it true} if the $i$-th letter in the document is an $a$. For instance, the document $d = abba$ is a structure with domain $A = \{1,2,3,4\}$ and predicates $P_a = \{1, 4\}$ and $P_b = \{2, 3\}$. This version of MSO has the following recursive syntax: First, we define atomic formulas $P_a(x)$, $x\in X$ and $x\leq y$ in which $x$ and $y$ are (first-order) variables that will later be evaluated as positions in an input document, $X$ is a (second-order) variable that will be evaluated as a set of positions, and $P_a(x)$ is true iff the $x$-th position in the document has the character $a$. 
%Now we assume we have (non-necessarily atomic) formulas $\varphi$ and $\psi$ and we can build the rest of the valid formulas in the logic: $\neg \varphi$, $\varphi \wedge \psi$, $\exists x .\, \varphi$, and $\exists X.\, \varphi$, where $x$ and $X$ are first- and second-order variables respectively. For instance $\varphi(X) = \neg\exists x.\,(P_a(x)\wedge \neg\  x\!\in\! X)$ evaluated on an input document $w$ is true iff $X$ contains all positions in $w$ where there is an $a$.
%MSO over documents is equivalent to regular expressions [citacitacita], and further, 

Having discussed some scenarios where one would want to extract data using queries that are based on regex and their extensions, we note that each used an ad-hoc extension of the yes/no query language to capture the desired outputs. We contrast this with MSO, which has the ability to define queries that extract sets of positions in an input document in a natural fashion. This is especially useful for us, as this ability implies there is a generic adaptation from yes/no evaluation into queries with several, complex outputs, for every extension of MSO.

%Regular Spanners can be directly expressed by an MSO extraction query, and were the basis of an early landmark in the literature of output-linear delay enumeration in Database Theory~\cite{FlorenzanoRUVV20}. It is not a stretch to say that this Thesis contains several extensions of this very result.

%Let us note that MSO treats second-order variables and predicates that indicate a letter indistinguishably  --- for a variable $x$, the former is notated ``$x\in X$'' and the latter ``$P_{a}(x)$''. Indeed, we can think of letters and variables simply as two dimensions over which we label the positions in a document.  
This idea is the core of the {\it Annotator} framework, which translates many rule-based binary query languages into query languages with complex outputs. This is done by keep using the same type of binary model that describes the query while adding a second dimension to the alphabet that describes the data---we use these as the symbols the model will treat as letters. For instance, the underlying binary model of an annotator may accept words such as  $(a,x)bb(a,y)$ or $a(b,x)(b,y)a$, and if we give the word $abba$ to the annotator, it will define an output set that includes the strings $(x,1)(y,4)$ and $(x,2)(y,3)$. The first string represents the idea that if we extend $abba$ by appending an $x$ to the first position, and a $y$ to the fourth position, the resulting word is accepted by the model, and the second string  represents the analogous idea.

We have found that some of the relevant models for document spanners that extend a binary query language into a complex one---as is the case for Regular Spanners and Extraction Grammars---can be reduced into an annotator with only a minor blowup. More importantly, we believe that this change makes the evaluation algorithms simpler, and that it is a reason behind our finding improvements in the best-known bounds for them.

The main goal of this thesis is thus to explore the power of the Annotator framework, and provide efficient enumeration algorithms for the ways it can be used to deal with known query languages. It is worth noting that while many of the query languages studied in the literature were restricted to outputs with a fixed size, annotators by default do not impose any such restriction, so all of our results are given in the best-possible delay bound that this circumstance allows; namely, output-linear delay.

%Recursively structured and quasi-structured data are
%
%
%Now we can discuss the query space that we will work with in this thesis. All of our results are given for some sort of extension or improvement on Monadic Second Order over strings or trees. This query language is equivalent to regular automata, and subsumes standard queries over JSON and XML documents when it is queried over trees. Further, it can be applied to relational and graph query databases, and other sorts of common query tasks.
%
%All the query tasks built around Document Spanners can be expressed by some extension of regular automata (the query), and a string (the data). A great part of the enumeration results for Document Spanners in the literature are described using the automata model, and this Thesis follows the same rationale to some extent.
%
%All of the aforementioned results deal in query languages that lack any strong form of recursion, namely, with the level of context-free grammars. The algorithmic properties of context-free grammars and pushdown automata have been widely studied, but any efficiently implementable queries have been limited to binary evaluation. The only enumeration result for CFG-modeled queries that we know of~\cite{peterfreund2019complexity} goes in the direction that we desire, although with a gap between the complexity of its binary counterpart. Our goal is to meet the enumeration results for automata and the binary decision algorithms for context-free grammars in the middle, and improve on any known results along the way.

To this end, in this thesis we present three different formalisms for information extraction and then provide an efficient enumeration scheme for queries expressed in each of them. 
These are: (1.) Automata over streams of nested documents, (2.) context-free grammars which represent annotations in documents and (3.) regular automata over compressed documents. 


%Enumeration algorithms are a refinement of evaluation algorithms in general. An evaluation problem describes the task of receiving an input and producing an output that satisfies some requirements. A typical evaluation algorithm would deal with this task by optimizing the time transcurred between receiving the input and producing the output. This is not helpful for a wide array of evaluation tasks that are of interest when processing large amounts of data. (ejemplo con numeros)



%Arguably the most fundamental problem in database research is query
%evaluation: given as input a query and data, we must find the results of the query over the data.
%Database theory research has studied the complexity of such problems for 
%decades.
%However, in some contexts, for instance over large datasets, the usual
%complexity measures are not well-suited to this study.
%Indeed, the number of query results might be so large that it is unreasonable in
%practice to produce all of them.
%Further, in theoretical terms, the complexity of an algorithm may be dominated by the cost of writing
%the full output, hiding the actual complexity of the
%computation.
%For this reason, a significant line of research on query evaluation has adopted the perspective of \emph{enumeration algorithms}.
%Instead of explicitly producing all results, the task is to \emph{enumerate} them, in any order and without repetition.
%The cost of the algorithm is then measured across two dimensions: the
%\emph{preprocessing time}, which is the time needed to read the input and
%prepare an enumeration data structure; and the \emph{delay}, the worst-case time
%elapsed between any two solutions while enumerating using the data structure.
%
%This study of enumeration algorithms has managed in some cases to achieve a
%\emph{constant-delay} guarantee. In this case, once the algorithm has
%preprocessed its input, the delay between any two outputs is \emph{constant},
%i.e., it is independent from the input. Of course, the challenge is to achieve
%this strong guarantee after a preprocessing phase that runs in a limited amount
%of time -- in particular, one that does not explicitly materialize all solutions.
%%
%Starting with the work of Durand and Grandjean~\cite{durand2007first}, researchers have designed
%constant-delay algorithms for several query evaluation problems, e.g., the evaluation of some queries over relational databases~\cite{bagan2007acyclic,kara2020trade}, query evaluation over dynamic data~\cite{berkholz2017answering,idris2017dynamic}, query evaluation over graph data~\cite{hartig2018semantics,kroll2016complexity}, among others~\cite{Segoufin13}. 
%
%One area where enumeration algorithms have been especially successful is the problem of \emph{information extraction}, studied through the lens of \emph{document spanners}~\cite{FaginKRV15}. In this data management task, the data is a textual document (i.e., a string), and the query is a declarative specification of information to extract from the text, formalized as a \emph{spanner}. The spanner describes \emph{mappings}, which are possible choices of how to map variables to substrings of the document (called spans). The enumeration problem is then to enumerate all mappings of a spanner on an input document, i.e., to enumerate efficiently all possible results for the information extraction task.
%%
%The work by Florenzano et al.~\cite{FlorenzanoRUVV18} showed that the task could be solved with preprocessing linear in the document and polynomial in a finite deterministic automaton describing the spanner, improving on a theoretical result by Bagan~\cite{bagan2006mso}; and 
%this was extended in~\cite{amarilli2019constant} to spanners described using nondeterministic automata or regular expressions.
%
%However, while regular spanners are natural, they do not capture all
%possible information extraction tasks. More
%expressiveness is needed for extraction over structured data (e.g., XML,
%or JSON documents), over the source code of programs, or possibly over
%natural language texts. 

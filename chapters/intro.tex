%1) Big data, efficient evaluation, linear time over the data. 
%
%2) Query evaluation, query languages Q, model this as Q(D). Evaluation in |D|.
%
%3) Q(D) is only one part of the story, also we need to talk about results. You want to evaluate and enumerate the output hopefully in time f(Q,D) + |O|.
%
%4) Talk about constant delay as a more efficient notion of enumeration guarantee.
%
%5) Let's instatiate this idea in a more concrete query language, specifically, in this work a good starting point is to talk about query lanuguages based on MSO. Hablar sobre MSO, XML, JSON, bases de datos de grafos, etc.
%
%6) Spanners como un ejemplo especial y importante para este trabajo. 
%
%7) (parráfo corto) Concluir que en todos estos casos, al final, la consulta se puede expresar como un automata (en algún modelo) y los datos como un string. 
%
%8) Evaluación eficient con enumeración de automatas ya se ha estudiado harto y teniendo grandes resultados. Por ejemplo, Bagan, Antoine, Spanners, CEP. 
%
%9) Todos los resultados anteriores no consideran algún nivel de recursión en los automatas como visibly, grammars. Es sabido que visibly y gramaticas también tienen buenas propiedades algoritmicas, pero esto es solo para el caso booleano. Creemos que podemos estos resultados a automatas o datos donde hay cierto nivel de recursión.

The term {\it Big Data} is believed to have been originated in the mid-1990s, back when the Internet was utilized by some tens of millions of users. Even then, as the usage of the term suggests, there was a distinct necessity of processing tremendous amounts of data efficiently. In 2023, this number grew to 5,4 billion users~\cite{netuse}, and that has naturally brought a great increase in the amount of raw data that is available -- and often required -- to be processed.
For example, Netflix reported in 2012 that their users queued 2 million movies and TV shows, and generated 4 million ratings a day~\cite{netflixsize}, data which is then fed to their recommendation algorithms. Nowadays, four out of the five most visited sites on the Internet use user-generated information to suggest content back to the users~\cite{topwebsites}. 
Besides recommendation, content management and moderation in contexts where millions of messages are being sent daily is crucial to curb undesirable data such as fake news~\cite{fakenews} and hate speech~\cite{hatespeech}.
These cases show how crucial, diverse and massive the online data processing tasks have become.

The blowup of large data processing has happened in the offline world as well. In biological research, human genomes consist of around three billion base pairs, which translate into several gigabytes of raw data that is processed when performing genome-matching or detecting genetic diseases~\cite{genomes}. Modern space missions, such as GAIA and LSST, are expected to produce Petabytes of data~\cite{space}. To name a few other examples, patents are analyzed in bulk in order to predict future litigations~\cite{patents}; GPS records from large populations can be used in urban planning projects~\cite{urban}; and historical crime data is massively processed to predict crime~\cite{crime}.

The importance and desirability of having adequate data processing solutions is unquestionable. In many cases, these algorithms need to be sharply efficient, to the point of making any solution that takes time longer than linear in the data size useless in practice.

In database systems, it is common to talk about {\it query evaluation} as the most elementary task which extracts relevant information from data. It can be formally defined by an instance $(Q, D)$ in which $Q$ is the query and $D$ is the data. Each query $Q$ also defines a function which maps $D$ into the desired set of outputs $Q(D)$.
As an example, consider the task of detecting if a chromosome, written as a sequence of bases, contains a certain pattern in it that may correspond to a particular disease. In this case, it would make sense to let $D$ be the sequence of bases written as a string, and $Q$ would be the pattern one wants to find. If the task is a yes/no question -- is the pattern $Q$ present in $D$? -- then $Q(D)$ can evaluate to either {\it true} or {\it false}, but if one wants to also know all of the positions in the sequence that match that pattern, $Q(D)$ would be a set that contains all such positions, and in the case there are none, it evaluates to an empty set.
With this formalization, one can talk about linear-time solutions as those which take time proportional to the size of $D$.

It is also useful to frame results around {\it query languages}, which define classes of queries by their syntax and semantics. For instance, one can solve the task above by using {\it regular expressions} as the query language (see~\cite{automatahandbook} for a precise definition): we can make $Q$ be the regular expression $r = \texttt{.}^\texttt{*}p\texttt{.}^\texttt{*}$ where $p$ is the pattern string. In this case, we are using a usual semantics for regular expressions which is that $r$ has to match the entire text; the sequence $\texttt{.}^\texttt{*}$ is a wildcard that matches any string, and so $r$ is a match if the data is equal to anything, followed by $p$, followed by anything. Another aspect of query languages that make them natural to work with is that each of them also comes with a certain expressive power. For example, no regular expression can define the query of checking if two strings are identical~\cite{automatahandbook}, and there are more powerful query languages that can. It is common that the same evaluation task requires less running time or memory space if one only changes the query language to a more restricted one, so it makes sense to build each algorithm for a language in particular.

When measuring the execution time of a evaluation algorithm, the classic notion is to count the number of steps taken by the algorithm from the moment it begins, until the last output is printed. Since algorithms are built for inputs of different sizes, we typically measure this execution time as a function of the size of the input.
This is a reasonable model whenever the overall output size is small, e.g., when the output is simply true or false, or a numeric value. However, for problems in which one can expect a large amount of outputs, which occur quite naturally in many data management problems, it makes sense to treat the output printing phase separately. Naturally, the best execution time one can hope for the output phase is linear in the size of the entire output set, so the overall running time that we will set as our goal is given by $|D| + |\text{Output}|$. Here we are using the standard notation $|x|$ to refer to the size of an adequate encoding of $x$.

In database systems, there are many cases in which an evaluation problem can have a large output set. For instance, one may be interested not only in all elements in the data that satisfy a certain condition, but in tuples or subsets of them. Or perhaps the data may not fit in memory: it could be given in a streaming fashion or it could be compressed, and this implies that even an output set that has linear-size in the raw data is huge. An obvious problem here is that producing all outputs can take unreasonably long, but a more subtle one is that when one measures the running time of an algorithm, the cost of writing the outputs hides the cost of actually doing the calculations to obtain them.
For this reason, a significant line
of research on query evaluation has adopted the perspective of {\it enumeration algorithms}.
Instead of explicitly producing all results, the task is to enumerate them, in any order and
without repetition. The cost of the algorithm is then measured across two dimensions:
the {\it preprocessing time}, which is the time needed to read the input and prepare an
enumeration data structure; and the {\it delay}, the worst-case time elapsed between any two
solutions while enumerating using the data structure.


In the realm of {\it enumeration algorithms}, there is an even finer complexity yardstick:
We say that an algorithm has {\it constant-delay} if the delay between any two consecutive outputs is constant. One can think of this as an enumeration phase in which the outputs are produced in a fixed pace, never stopping until the last output is produced. However, this delay guarantee is not a reasonable one unless every output is forced to have constant size. In this case, it makes more sense to set as a goal what we call {\it output-linear delay}. This is a relaxation of constant-delay which only requires the delay between two outputs to be linear in the size of the former.

As an example, consider the task of receiving a list of numbers, such as $L = [5, 7, 2, 12]$, and producing a list of pairs $(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)$ in which $x_1,x_2,\ldots,x_n$ is the sorted list, and each $y_i$ is the difference between $x_i$ and $x_{i+1}$. For the said $L$, the desired set of outputs could be $\{(2,3), (5,2), (7,5),(12,\infty)\}$. A constant-delay algorithm may simply sort the list, read it sequentially and produce each value along with the difference. Now, consider the task of again receiving a list of numbers, but now instead of printing the difference between two numbers, we ask for the numbers that are present in that gap. For the input $L$, the output set would be $\{(2,3,4),(5,6),(7,8,9,10,11),(12,\infty)\}$. In this case, there is no reasonable way of solving this with a constant-delay algorithm, since the size of a single output could be arbitrarily large, even for a list of size 2. However, sorting the list and writing down the gaps is a perfectly valid solution and it satisfies output-linear delay.

In many practical cases, data is contained in documents that have a pre-established recursive structure, which may be hierarchical -- like XML and JSON documents -- or program-like -- such as some forms of compressed text [REFERENCIA A SLPS y LZ77]. These documents are designed to be read by computers, and the human's role is to write the query in a suitable language. When a single document is huge, and queries are simple enough, it makes sense to use a language based on regular expressions, such as XPath in the case of XML, or regex themselves for compressed text. As an example for XML, consider a document that contains information about writers and their works using the hierarchy {\tt writer->book->[name, year, etc]}. An XPath query that produces all books published in the year 1984 might look as follows: $Q = \texttt{//writer/book[year = 1984]}$. For compressed text, regular expressions are not sought after only for the simplicity of the syntax but also of the evaluation, which can be translated straightforwardly to work on the compressed file itself [REFERENCIA A SLP y regex].

Document Spanners [cita benny] is a specially relevant model for queries based on regular expressions. In the area of Information Extraction -- the field of blabl ablblablabla -- the sub-area of rule-based Information Extraction studies bla blabl bla bl bal, where Document Spanners were proposed as a suitable formalism.  In its more fundamental definition, a Document Spanner is a function that maps a document to an output set that contains tuples of {\it spans}, which are pairs of positions in the document. They can easily model tasks of converting plain or semi-structured text into a relational database. Inside these, we find {\it Regular Spanners}, which are defined by extending regular expressions (or finite-state automata) with symbols that mark the beginning and ending of a span. A more powerful model are the {\it Core Spanners}, which extend Regular Spanners with operators from relational algebra, such as projection, complement, selection of a spanner and joins between them. 

Efficient enumeration for queries based on Regular Spanners has found great success in relatively recent times. A relevant precursor was the result given in~\cite{bagan2006mso}, which describes how to enumerate the valid assignments for a given MSO formula --- onto which Regular Spanners can be trivially reduced. The papers~\cite{FlorenzanoRUVV20} and~\cite{amarilli2019constant} give parallel results which improve the delay to linear in the {\it support} of the assignments. The work in~\cite{Niewerth18} had a log-delay result for MSO queries over trees, which was later improved to output-linear delay~\cite{amarilli2019enumeration}. 

In cases where the document does not have a pre-established structure, but one can be inferred from it, grammar-based queries are a popular option. Some scenarios in which this type of queries have been famously used are code verification and [blabla]. One model that has been proposed to deal with these queries is based on Document Spanners and is called Extraction Grammars [cita liat]. These are defined by context-free grammars which allow beginning and ending position marks in its syntax. 
In same work, we also find a relevant enumeration result. It shows constant-delay enumeration after a preprocessing that takes quintic time on the size of the document.

Complex Event Processing is another realm that is relevant to our work. Here, the goal is to blablballbalbla. Some enumeration algorithms for queries in this area have also been realized by compiling them into a suitable automaton~\cite{GrezRU19,GrezR20,BucchiGQRV22}.


Queries based on regular expressions and automata can be understood under the umbrella of Monadic Second Order logic. 
A version of this logic that is restricted to work on documents would represent a document of size $n$ as a structure with domain $\{1,\ldots, n\}$ and predicates $P_a$ that evaluate $P_a(i)$ as {\it true} if the $i$-th letter in the document is an $a$. For instance, the document $d = abba$ is a structure with domain $A = \{1,2,3,4\}$ and predicates $P_a = \{1, 4\}$ and $P_b = \{2, 3\}$. This version of MSO has the following recursive syntax: First, we define atomic formulas $P_a(x)$, $x\in X$ and $x\leq y$ in which $x$ and $y$ are (first-order) variables that will later be evaluated as positions in an input document, $X$ is a (second-order) variable that will be evaluated as a set of positions, and $P_a(x)$ is true iff the $x$-th position in the document has the character $a$. 
Now we assume we have (non-necessarily atomic) formulas $\varphi$ and $\psi$ and we can build the rest of the valid formulas in the logic: $\neg \varphi$, $\varphi \wedge \psi$, $\exists x .\, \varphi$, and $\exists X.\, \varphi$, where $x$ and $X$ are first- and second-order variables respectively. For instance $\varphi(X) = \neg\exists x.\,(P_a(x)\wedge \neg\  x\!\in\! X)$ evaluated on an input document $w$ is true iff $X$ contains all positions in $w$ where there is an $a$.
MSO over documents is equivalent to regular expressions [citacitacita], and further, it has the ability to define queries that extract the desired positions, and sets of positions in the document in a natural fashion. This is especially useful for us, as this is a seamless way of extending regular expression-based queries from yes/no evaluation into queries with several, complex outputs.

%Regular Spanners can be directly expressed by an MSO extraction query, and were the basis of an early landmark in the literature of output-linear delay enumeration in Database Theory~\cite{FlorenzanoRUVV20}. It is not a stretch to say that this Thesis contains several extensions of this very result.

Let us note that MSO treats second-order variables and predicates that indicate a letter indistinguishably  --- for a variable $x$, the former is notated ``$x\in X$'' and the latter ``$P_{a}(x)$''. Indeed, we can think of letters and variables simply as two dimensions over which we label the positions in a document.  The idea above is the core of the {\it Annotator} framework, which translates many rule-based binary query languages into query languages with complex outputs. This is done by keep using the same type of binary model that describes the query while adding a second dimension to the alphabet that describes the data --- we use these as the symbols the model will treat as letters. For instance, the underlying binary model of an annotator may accept words such as  $(a,x)bb(a,y)$ or $a(b,x)(b,y)a$, and if we give the word $abba$ to the annotator, it will define an output set that includes the strings $(x,1)(y,4)$ and $(x,2)(y,3)$. The first string represents the idea that if we extend $abba$ by appending an $x$ to the first position, and a $y$ to the fourth position, the resulting word is accepted by the model, and the second string  represents the analogous idea.

We have found that some of the relevant models for Document Spanners that extend a binary query language into a complex one --- as is the case for Regular Spanners and Extraction Grammars --- can be reduced into an annotator with only a minor blowup. More importantly, we believe that this change makes the evaluation algorithms simpler, and that it is a reason behind our finding improvements in the best known bounds for them.

The main goal of this thesis is thus to explore the power of the Annotator framework, and provide efficient enumeration algorithms for the ways it can be used to deal with known query languages. It is worth noting that while many of the query languages studied in the literature were restricted to outputs with a fixed size, annotators by default do not impose any such restriction, so all of our results are given in the best-possible delay bound that this circumstance allows; namely, output-linear delay.

%Recursively structured and quasi-structured data are
%
%
%Now we can discuss the query space that we will work with in this thesis. All of our results are given for some sort of extension or improvement on Monadic Second Order over strings or trees. This query language is equivalent to regular automata, and subsumes standard queries over JSON and XML documents when it is queried over trees. Further, it can be applied to relational and graph query databases, and other sorts of common query tasks.
%
%All the query tasks built around Document Spanners can be expressed by some extension of regular automata (the query), and a string (the data). A great part of the enumeration results for Document Spanners in the literature are described using the automata model, and this Thesis follows the same rationale to some extent.
%
%All of the aforementioned results deal in query languages that lack any strong form of recursion, namely, with the level of context-free grammars. The algorithmic properties of context-free grammars and pushdown automata have been widely studied, but any efficiently implementable queries have been limited to binary evaluation. The only enumeration result for CFG-modeled queries that we know of~\cite{peterfreund2019complexity} goes in the direction that we desire, although with a gap between the complexity of its binary counterpart. Our goal is to meet the enumeration results for automata and the binary decision algorithms for context-free grammars in the middle, and improve on any known results along the way.

To this end, In this thesis we present three different formalisms for information extraction and then provide an efficient enumeration scheme for queries expressed in each of them. 
These are: (1.) Automata over streams of nested documents, (2.) context-free grammars which represent annotations in documents and (3.) regular automata over compressed documents. 


%Enumeration algorithms are a refinement of evaluation algorithms in general. An evaluation problem describes the task of receiving an input and producing an output that satisfies some requirements. A typical evaluation algorithm would deal with this task by optimizing the time transcurred between receiving the input and producing the output. This is not helpful for a wide array of evaluation tasks that are of interest when processing large amounts of data. (ejemplo con numeros)



%Arguably the most fundamental problem in database research is query
%evaluation: given as input a query and data, we must find the results of the query over the data.
%Database theory research has studied the complexity of such problems for 
%decades.
%However, in some contexts, for instance over large datasets, the usual
%complexity measures are not well-suited to this study.
%Indeed, the number of query results might be so large that it is unreasonable in
%practice to produce all of them.
%Further, in theoretical terms, the complexity of an algorithm may be dominated by the cost of writing
%the full output, hiding the actual complexity of the
%computation.
%For this reason, a significant line of research on query evaluation has adopted the perspective of \emph{enumeration algorithms}.
%Instead of explicitly producing all results, the task is to \emph{enumerate} them, in any order and without repetition.
%The cost of the algorithm is then measured across two dimensions: the
%\emph{preprocessing time}, which is the time needed to read the input and
%prepare an enumeration data structure; and the \emph{delay}, the worst-case time
%elapsed between any two solutions while enumerating using the data structure.
%
%This study of enumeration algorithms has managed in some cases to achieve a
%\emph{constant-delay} guarantee. In this case, once the algorithm has
%preprocessed its input, the delay between any two outputs is \emph{constant},
%i.e., it is independent from the input. Of course, the challenge is to achieve
%this strong guarantee after a preprocessing phase that runs in a limited amount
%of time -- in particular, one that does not explicitly materialize all solutions.
%%
%Starting with the work of Durand and Grandjean~\cite{durand2007first}, researchers have designed
%constant-delay algorithms for several query evaluation problems, e.g., the evaluation of some queries over relational databases~\cite{bagan2007acyclic,kara2020trade}, query evaluation over dynamic data~\cite{berkholz2017answering,idris2017dynamic}, query evaluation over graph data~\cite{hartig2018semantics,kroll2016complexity}, among others~\cite{Segoufin13}. 
%
%One area where enumeration algorithms have been especially successful is the problem of \emph{information extraction}, studied through the lens of \emph{document spanners}~\cite{FaginKRV15}. In this data management task, the data is a textual document (i.e., a string), and the query is a declarative specification of information to extract from the text, formalized as a \emph{spanner}. The spanner describes \emph{mappings}, which are possible choices of how to map variables to substrings of the document (called spans). The enumeration problem is then to enumerate all mappings of a spanner on an input document, i.e., to enumerate efficiently all possible results for the information extraction task.
%%
%The work by Florenzano et al.~\cite{FlorenzanoRUVV18} showed that the task could be solved with preprocessing linear in the document and polynomial in a finite deterministic automaton describing the spanner, improving on a theoretical result by Bagan~\cite{bagan2006mso}; and 
%this was extended in~\cite{amarilli2019constant} to spanners described using nondeterministic automata or regular expressions.
%
%However, while regular spanners are natural, they do not capture all
%possible information extraction tasks. More
%expressiveness is needed for extraction over structured data (e.g., XML,
%or JSON documents), over the source code of programs, or possibly over
%natural language texts. 
